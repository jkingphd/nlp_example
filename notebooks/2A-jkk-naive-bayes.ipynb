{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2A-jkk-naive-bayes\n",
    "\n",
    "In this notebook, we will train a [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html) classifier using a bag-of-words representation of text. The goal of this notebooks is to introduce the following scikit-learn classes: [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?highlight=countvectorizer#sklearn.feature_extraction.text.CountVectorizer), [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer), [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB), [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV), and [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV).\n",
    "\n",
    "Note that the documentation linked above may refer to a newer version of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "seed = 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>num</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>avg</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>This:\\n:One can make an analogy in mathematica...</td>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4216.0</td>\n",
       "      <td>\"\\n\\n:Clarification for you  (and Zundark's ri...</td>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8953.0</td>\n",
       "      <td>Elected or Electoral? JHK</td>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26547.0</td>\n",
       "      <td>\"This is such a fun entry.   Devotchka\\n\\nI on...</td>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28959.0</td>\n",
       "      <td>Please relate the ozone hole to increases in c...</td>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rev_id                                            comment  year  \\\n",
       "0   2232.0  This:\\n:One can make an analogy in mathematica...  2002   \n",
       "1   4216.0  \"\\n\\n:Clarification for you  (and Zundark's ri...  2002   \n",
       "2   8953.0                          Elected or Electoral? JHK  2002   \n",
       "3  26547.0  \"This is such a fun entry.   Devotchka\\n\\nI on...  2002   \n",
       "4  28959.0  Please relate the ozone hole to increases in c...  2002   \n",
       "\n",
       "   logged_in       ns  sample  split   num  min  max  avg  y  \n",
       "0          1  article  random  train  10.0 -1.0  1.0  0.4  0  \n",
       "1          1     user  random  train  10.0  0.0  2.0  0.5  0  \n",
       "2          0  article  random   test  10.0  0.0  1.0  0.1  0  \n",
       "3          1  article  random  train  10.0  0.0  2.0  0.6  0  \n",
       "4          1  article  random   test  10.0 -1.0  1.0  0.2  0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with sql.connect('../data/toxic.db') as conn:\n",
    "    df = pd.read_sql_query('select * from toxic', conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to isolate the train, dev, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = df['split'] == 'train'\n",
    "idx_dev = df['split'] == 'dev'\n",
    "idx_test = df['split'] == 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off by training a CountVectorizer, which is a nice class provided by Scikit-learn that allows you to transform raw text into a sparse token count vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(\n",
    "    token_pattern = r\"[a-z]+\", \n",
    "    ngram_range = (1,1),\n",
    "    lowercase = True,\n",
    "    min_df = 1,\n",
    "    max_df = 1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<95692x124461 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4277363 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df.loc[idx_train, 'comment'].values\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "X_train_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the fit_transform method allows us to fit the CountVectorizer (build a dictionary from the select parameters) and transform the training set into a sparse matrix representing token counts. Each row is an instance from the training set, and each column is associated with a token. The value carried represents the number of times a particular token occurred in the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This:\\n:One can make an analogy in mathematical terms by envisioning the distribution of opinions in a population as a Gaussian curve. We would then say that the consensus would be a statement that represents the range of opinions within perhaps three standard deviations of the mean opinion. \\nsounds arbitrary and ad hoc.  Does it really belong in n encyclopedia article?  I don't see that it adds anything useful.\\n\\nThe paragraph that follows seems much more useful.  Are there any political theorists out there who can clarify the issues?  It seems to me that this is an issue that Locke, Rousseau, de Toqueville, and others must have debated...  SR\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x124461 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 83 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vect[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the output by using instance variables created by the fit/fit_transform method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t3\ta\n",
      "1\t1\tad\n",
      "2\t1\tadds\n",
      "3\t2\tan\n",
      "4\t1\tanalogy\n",
      "5\t2\tand\n",
      "6\t1\tany\n",
      "7\t1\tanything\n",
      "8\t1\tarbitrary\n",
      "9\t1\tare\n",
      "10\t1\tarticle\n",
      "11\t1\tas\n",
      "12\t1\tbe\n",
      "13\t1\tbelong\n",
      "14\t1\tby\n",
      "15\t2\tcan\n",
      "16\t1\tclarify\n",
      "17\t1\tconsensus\n",
      "18\t1\tcurve\n",
      "19\t1\tde\n",
      "20\t1\tdebated\n",
      "21\t1\tdeviations\n",
      "22\t1\tdistribution\n",
      "23\t1\tdoes\n",
      "24\t1\tdon\n",
      "25\t1\tencyclopedia\n",
      "26\t1\tenvisioning\n",
      "27\t1\tfollows\n",
      "28\t1\tgaussian\n",
      "29\t1\thave\n",
      "30\t1\thoc\n",
      "31\t1\ti\n",
      "32\t3\tin\n",
      "33\t1\tis\n",
      "34\t1\tissue\n",
      "35\t1\tissues\n",
      "36\t3\tit\n",
      "37\t1\tlocke\n",
      "38\t1\tmake\n",
      "39\t1\tmathematical\n",
      "40\t1\tme\n",
      "41\t1\tmean\n",
      "42\t1\tmore\n",
      "43\t1\tmuch\n",
      "44\t1\tmust\n",
      "45\t1\tn\n",
      "46\t3\tof\n",
      "47\t1\tone\n",
      "48\t1\topinion\n",
      "49\t2\topinions\n",
      "50\t1\tothers\n",
      "51\t1\tout\n",
      "52\t1\tparagraph\n",
      "53\t1\tperhaps\n",
      "54\t1\tpolitical\n",
      "55\t1\tpopulation\n",
      "56\t1\trange\n",
      "57\t1\treally\n",
      "58\t1\trepresents\n",
      "59\t1\trousseau\n",
      "60\t1\tsay\n",
      "61\t1\tsee\n",
      "62\t2\tseems\n",
      "63\t1\tsounds\n",
      "64\t1\tsr\n",
      "65\t1\tstandard\n",
      "66\t1\tstatement\n",
      "67\t1\tt\n",
      "68\t1\tterms\n",
      "69\t6\tthat\n",
      "70\t6\tthe\n",
      "71\t1\tthen\n",
      "72\t1\ttheorists\n",
      "73\t2\tthere\n",
      "74\t2\tthis\n",
      "75\t1\tthree\n",
      "76\t1\tto\n",
      "77\t1\ttoqueville\n",
      "78\t2\tuseful\n",
      "79\t1\twe\n",
      "80\t1\twho\n",
      "81\t1\twithin\n",
      "82\t2\twould\n"
     ]
    }
   ],
   "source": [
    "id2token = {v:k for k,v in vect.vocabulary_.items()}\n",
    "counter = 0\n",
    "for i, value in enumerate(X_train_vect[0].toarray()[0]):\n",
    "    if value != 0:\n",
    "        print(f\"{counter}\\t{value}\\t{id2token[i]}\")\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a vectorized training set, we should be able to train a Naive Bayes model and test performance against the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "y_train = df.loc[idx_train, 'y'].values\n",
    "\n",
    "clf = MultinomialNB() # Leave everything at default for now\n",
    "clf.fit(X_train_vect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = df.loc[idx_dev, 'comment'].values\n",
    "X_dev_vect = vect.transform(X_dev) # Note that the vectorizer is already fit, so we only use the transform method.\n",
    "y_dev = df.loc[idx_dev, 'y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8967255976095617"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_dev_vect)\n",
    "np.mean(y_dev==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we nearly got 90% on the dev set. Is accuracy a good metric for this problem though?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25186,  1064],\n",
       "       [ 2254,  3624]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_dev, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94     26250\n",
      "           1       0.77      0.62      0.69      5878\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     32128\n",
      "   macro avg       0.85      0.79      0.81     32128\n",
      "weighted avg       0.89      0.90      0.89     32128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_dev, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall is pretty poor for the positive class. Let's take a look at a situation where we incorrectly classified a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed from article: \n",
      ":An extra-large common pork sausage was named after him.  \n",
      "\n",
      "Nah.  \n"
     ]
    }
   ],
   "source": [
    "idx_error = (y_dev != y_pred) & (y_dev == 1)\n",
    "print(X_dev[idx_error][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's strange, what was the average toxicity score for this instance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>num</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>avg</th>\n",
       "      <th>y</th>\n",
       "      <th>hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>2943468.0</td>\n",
       "      <td>Removed from article: \\n:An extra-large common...</td>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>-7357645735245327406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rev_id                                            comment  year  \\\n",
       "445  2943468.0  Removed from article: \\n:An extra-large common...  2003   \n",
       "\n",
       "     logged_in       ns  sample split   num  min  max  avg  y  \\\n",
       "445          1  article  random   dev  10.0 -1.0  1.0 -0.1  1   \n",
       "\n",
       "                    hash  \n",
       "445 -7357645735245327406  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['hash'] = df['comment'].map(hash) # Using hash to speed up lookup\n",
    "df[df['hash'] == hash(X_dev[idx_error][1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may indicate problems in the dataset (or how we aggregate the scores). At this point, I would normally revisit the EDA step to review the labeling function, but let's forge ahead for now. Since we have a pretty large imbalance and the positive class is low, let's use F1 as the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6859738784781374"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_dev, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "So how do we efficiently tune our hyperparameters? We can use GridSearchCV or RandomizedSearchCV, but we have a pre-defined dev set so we need to use some tricks to override the normal behavior. This is actually pretty standard for large-scale NLP problems. Cross-validation is preferred, but often not feasible for large datasets.\n",
    "\n",
    "```\n",
    "For some datasets, a pre-defined split of the data into training- and validation fold or into several cross-validation folds already exists. Using PredefinedSplit it is possible to use these folds e.g. when searching for hyperparameters.\n",
    "\n",
    "For example, when using a validation set, set the test_fold to 0 for all samples that are part of the validation set, and to -1 for all other samples.\n",
    "```\n",
    "\n",
    "With that in mind, we can set up a PredefinedSplit. Note that the following code is bad from a memory standpoint. We are simply doing it this way for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = df.loc[idx_train, \"comment\"].values\n",
    "y_train = df.loc[idx_train, \"y\"].values\n",
    "\n",
    "X_dev = df.loc[idx_dev, \"comment\"].values\n",
    "y_dev = df.loc[idx_dev, \"y\"].values\n",
    "\n",
    "X = np.hstack([X_train, X_dev])\n",
    "y = np.hstack([y_train, y_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0    95692\n",
       " 0.0    32128\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.zeros(shape=y.shape)\n",
    "idx[:y_train.shape[0]] = -1\n",
    "pd.value_counts(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "\n",
    "ps = PredefinedSplit(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, let's talk about pipelines. Pipelines are a helper class in scikit-learn that allow you daisy-chain transformers and classifiers together into a single object. This can vastly simply training and deployment. As an example, let's look at our earlier example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='[a-z]+', tokenizer=None,\n",
       "        vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vect = CountVectorizer(\n",
    "    token_pattern = r\"[a-z]+\", \n",
    "    ngram_range = (1,1),\n",
    "    lowercase = True,\n",
    "    min_df = 1,\n",
    "    max_df = 1.0\n",
    ")\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "pipe = Pipeline([(\"vect\", vect), (\"clf\", clf)])\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(X_dev[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Pipeline and GridSearchCV, we can define a parameter range to characterize. GridSearchCV will keep track of the parameters used and the target metric (F1 here), and report the best performing combination. Note that since we defined the basic pipeline above, we do not need to redefine it before tuning. Each iteration will overwrite the previous configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 30 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  30 out of  30 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__fit_prior': True, 'vect__min_df': 2, 'vect__ngram_range': (1, 1)}\n",
      "0.690830822212656\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'vect__ngram_range':[(1,1), (1,2), (1,3)],\n",
    "    'vect__min_df':[1, 2, 5, 10, 20],\n",
    "    'clf__fit_prior':[False, True]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid, scoring='f1', n_jobs=6, cv=ps, verbose=2)\n",
    "gs.fit(X, y)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this step took so long, I am going to persist the result sto disk using joblib (cooking-show style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../results/gs_cv_nb.joblib']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "dump(gs, '../results/gs_cv_nb.joblib')\n",
    "# gs = load('../results/gs_cv_nb.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24515,  1735],\n",
       "       [ 1473,  4405]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = gs.best_estimator_.predict(X_dev)\n",
    "confusion_matrix(y_dev, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94     26250\n",
      "           1       0.72      0.75      0.73      5878\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     32128\n",
      "   macro avg       0.83      0.84      0.84     32128\n",
      "weighted avg       0.90      0.90      0.90     32128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9001494023904383"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_dev==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a bad result! Let's see if adding a TfidfVectorizer can make a difference by adding it to the parameter search. TF-IDF will reduce the weight of high-frequency words and increase the weight of rare words (via inverse document frequency, or IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect_1 = CountVectorizer(\n",
    "    token_pattern = r\"[a-z]+\", \n",
    "    ngram_range = (1,1),\n",
    "    lowercase = True,\n",
    "    min_df = 1,\n",
    "    max_df = 1.0\n",
    ")\n",
    "\n",
    "vect_2 = TfidfVectorizer(\n",
    "    token_pattern = r\"[a-z]+\", \n",
    "    ngram_range = (1,1),\n",
    "    lowercase = True,\n",
    "    min_df = 1,\n",
    "    max_df = 1.0\n",
    ")\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "pipe = Pipeline([(\"vect\", vect), (\"clf\", clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 60 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=6)]: Done  60 out of  60 | elapsed: 10.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__fit_prior': True, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='[a-z]+', tokenizer=None,\n",
      "        vocabulary=None), 'vect__min_df': 2, 'vect__ngram_range': (1, 1)}\n",
      "0.690830822212656\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'vect':[vect_1, vect_2],\n",
    "    'vect__ngram_range':[(1,1), (1,2), (1,3)],\n",
    "    'vect__min_df':[1, 2, 5, 10, 20],\n",
    "    'clf__fit_prior':[False, True]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid, scoring='f1', n_jobs=6, cv=ps, verbose=2)\n",
    "gs.fit(X, y)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same result! TF-IDF did not help here, which is an important lesson when it comes to machine learning (i.e., no free lunch). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24515,  1735],\n",
       "       [ 1473,  4405]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = gs.best_estimator_.predict(X_dev)\n",
    "confusion_matrix(y_dev, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94     26250\n",
      "           1       0.72      0.75      0.73      5878\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     32128\n",
      "   macro avg       0.83      0.84      0.84     32128\n",
      "weighted avg       0.90      0.90      0.90     32128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9001494023904383"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_dev==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you'll note that GridSearchCV is an exhaustive search across all possible parameters. Rather than define a rigid grid of points to examine, we can use RandomizedSearchCV to set a budget and sample from a parameter space. Let's add another transformation and more hyperparameters to tune as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "\n",
    "vect_1 = CountVectorizer(\n",
    "    token_pattern = r\"[a-z]+\", \n",
    "    ngram_range = (1,1),\n",
    "    lowercase = True,\n",
    "    min_df = 1,\n",
    "    max_df = 1.0\n",
    ")\n",
    "\n",
    "vect_2 = TfidfVectorizer(\n",
    "    token_pattern = r\"[a-z]+\", \n",
    "    ngram_range = (1,1),\n",
    "    lowercase = True,\n",
    "    min_df = 1,\n",
    "    max_df = 1.0\n",
    ")\n",
    "\n",
    "select = SelectPercentile(score_func=chi2)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "pipe = Pipeline([(\"vect\", vect), (\"select\", select), (\"clf\", clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while there are 2\\*3\\*5\\*6\\*2=360 parameter combinations, we are only going to run 30 trials drawn randomly from the possibility space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 30 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  30 out of  30 | elapsed:  6.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vect__ngram_range': (1, 2), 'vect__min_df': 2, 'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='[a-z]+', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'select__percentile': 5, 'clf__fit_prior': False}\n",
      "0.7009715725080965\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'vect':[vect_1, vect_2],\n",
    "    'vect__ngram_range':[(1,1), (1,2), (1,3)],\n",
    "    'vect__min_df':[1, 2, 5, 10, 20],\n",
    "    'select__percentile':[1, 2, 5, 10, 20, 50],\n",
    "    'clf__fit_prior':[False, True]\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(pipe, param_grid, n_iter=30, scoring='f1', n_jobs=6, cv=ps, verbose=2)\n",
    "rs.fit(X, y)\n",
    "print(rs.best_params_)\n",
    "print(rs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../results/rs_cv_nb.joblib']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(gs, '../results/rs_cv_nb.joblib')\n",
    "# gs = load('../results/rs_cv_nb.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25047,  1203],\n",
       "       [ 1392,  4486]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rs.best_estimator_.predict(X_dev)\n",
    "confusion_matrix(y_dev, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95     26250\n",
      "           1       0.79      0.76      0.78      5878\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     32128\n",
      "   macro avg       0.87      0.86      0.86     32128\n",
      "weighted avg       0.92      0.92      0.92     32128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9192293326693227"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_dev==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_intro]",
   "language": "python",
   "name": "conda-env-nlp_intro-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
